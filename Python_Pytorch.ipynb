{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9f4edd",
   "metadata": {},
   "source": [
    "# **Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e0fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135c395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dari list :\n",
      "tensor([1., 2., 3.])\n",
      "\n",
      "Tensor kosong :\n",
      "tensor([[-7.7219e+02,  1.5442e-42,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "\n",
      "Tensor dengan nol :\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "Tensor dengan satu :\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Tensor dengan distribusi normal :\n",
      "tensor([[ 1.9021, -0.7163,  0.6311],\n",
      "        [-0.8309,  0.3441, -0.0722],\n",
      "        [-2.0432, -0.8713,  0.0981]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor dari list\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Tensor kosong (nilai acak)\n",
    "x_empty = torch.empty(2, 3)\n",
    "\n",
    "# Tensor dengan nol\n",
    "x_zeros = torch.zeros(2, 2)\n",
    "\n",
    "# Tensor dengan satu\n",
    "x_ones = torch.ones(2, 2)\n",
    "\n",
    "# Tensor dengan distribusi normal\n",
    "x_rand = torch.randn(3, 3)\n",
    "\n",
    "print(f'Tensor dari list :\\n{x}\\n')\n",
    "print(f'Tensor kosong :\\n{x_empty}\\n')\n",
    "print(f'Tensor dengan nol :\\n{x_zeros}\\n')\n",
    "print(f'Tensor dengan satu :\\n{x_ones}\\n')\n",
    "print(f'Tensor dengan distribusi normal :\\n{x_rand}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf26de9",
   "metadata": {},
   "source": [
    "**Operasi Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bde5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6])\n",
      "tensor([3, 8])\n",
      "tensor(11.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "\n",
    "# Penjumlahan\n",
    "print(a + b)\n",
    "# Perkalian elemen-wise\n",
    "print(a * b)\n",
    "# Dot product\n",
    "print(torch.dot(a.float(), b.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d870da",
   "metadata": {},
   "source": [
    "**Autograd (Automatic Differentiation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141ebaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2 + 3 * x + 1\n",
    "y.backward()  # Hitung turunan\n",
    "print(x.grad)  # dy/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873784",
   "metadata": {},
   "source": [
    "# **Atribut**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58f46c",
   "metadata": {},
   "source": [
    "**Layer**\n",
    "| No | Layer                                       | Kategori        | Fungsi Umum                                                                 | Contoh Penggunaan                               |\n",
    "| -- | ------------------------------------------- | --------------- | --------------------------------------------------------------------------- | ----------------------------------------------- |\n",
    "| 1  | `nn.Linear`                                 | Fully Connected | Layer terhubung penuh. Digunakan hampir di semua model neural network.      | Output layer klasifikasi atau regresi           |\n",
    "| 2  | `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`          | Aktivasi        | Menambahkan fungsi aktivasi secara eksplisit.                               | Aktivasi non-linear (ReLU, sigmoid, dll)        |\n",
    "| 3  | `nn.Dropout`                                | Regularization  | Mencegah overfitting dengan menonaktifkan neuron secara acak saat training. | Model training agar tidak overfit               |\n",
    "| 4  | `nn.Flatten`                                | Reshaping       | Mengubah tensor menjadi vektor 1D.                                          | Dari output CNN ke Dense layer                  |\n",
    "| 5  | `torch.nn.Identity()` (sebagai dummy input) | Input Layer     | Tidak wajib eksplisit, biasanya didefinisikan dalam `forward`.              | Input data ke model                             |\n",
    "| 6  | `nn.Conv2d`                                 | Convolution     | Ekstraksi fitur dari gambar.                                                | Model CNN untuk pengenalan citra                |\n",
    "| 7  | `nn.MaxPool2d`                              | Pooling         | Mengurangi ukuran fitur map, mempertahankan fitur penting.                  | Setelah Conv2d untuk downsampling               |\n",
    "| 8  | `nn.BatchNorm1d` / `nn.BatchNorm2d`         | Normalization   | Menstabilkan dan mempercepat pelatihan.                                     | Setelah Conv2d atau Linear                      |\n",
    "| 9  | `nn.LSTM`                                   | Recurrent       | Mengingat urutan data dalam jangka panjang.                                 | Time series, NLP, prediksi urutan               |\n",
    "| 10 | `nn.Embedding`                              | NLP             | Mengubah kata (indeks) jadi vektor makna.                                   | Model teks seperti sentiment analysis           |\n",
    "| 11 | `torch.cat()`                               | Merging         | Menggabungkan beberapa tensor (fitur).                                      | Model dengan input atau output lebih dari satu  |\n",
    "| 12 | `torch.reshape()` / `.view()`               | Reshaping       | Mengubah bentuk tensor tanpa mengubah datanya.                              | Membentuk ulang input ke format yang diinginkan |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e455bbd",
   "metadata": {},
   "source": [
    "**Fungsi Aktivasi**\n",
    "| Nama Aktivasi  | Cara Memanggil (OOP)   | Cara Memanggil (Functional) | Kegunaan                                         |\n",
    "| -------------- | ---------------------- | --------------------------- | ------------------------------------------------ |\n",
    "| ReLU           | `nn.ReLU()`            | `F.relu(x)`                 | Default non-linear, cepat dan umum               |\n",
    "| Leaky ReLU     | `nn.LeakyReLU(0.01)`   | `F.leaky_relu(x, 0.01)`     | ReLU dengan grad negatif kecil (anti-dying ReLU) |\n",
    "| Sigmoid        | `nn.Sigmoid()`         | `torch.sigmoid(x)`          | Output 0–1, cocok untuk klasifikasi biner        |\n",
    "| Tanh           | `nn.Tanh()`            | `torch.tanh(x)`             | Output -1 sampai 1, sering dipakai di RNN        |\n",
    "| Softmax        | `nn.Softmax(dim=1)`    | `F.softmax(x, dim=1)`       | Klasifikasi multi-kelas (dipakai di output)      |\n",
    "| LogSoftmax     | `nn.LogSoftmax(dim=1)` | `F.log_softmax(x, dim=1)`   | Digunakan sebelum `NLLLoss()`                    |\n",
    "| GELU           | `nn.GELU()`            | `F.gelu(x)`                 | Smooth ReLU, dipakai di Transformer (BERT, GPT)  |\n",
    "| ELU            | `nn.ELU()`             | `F.elu(x)`                  | ReLU alternatif dengan output negatif smooth     |\n",
    "| Swish (custom) | `x * torch.sigmoid(x)` | -                           | Aktivasi self-gated, mirip GELU                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a9d54",
   "metadata": {},
   "source": [
    "**Fungsi Loss**\n",
    "| Nama Loss               | Cara Memanggil                       | Kegunaan                                                            |\n",
    "| ----------------------- | ------------------------------------ | ------------------------------------------------------------------- |\n",
    "| Mean Squared Error      | `nn.MSELoss()`                       | Regresi, menghitung rata-rata kuadrat error                         |\n",
    "| Mean Absolute Error     | `nn.L1Loss()`                        | Regresi, lebih tahan outlier dibanding MSE                          |\n",
    "| Binary Cross Entropy    | `nn.BCELoss()`                       | Klasifikasi biner (output pakai `sigmoid`)                          |\n",
    "| Binary CE with Logits   | `nn.BCEWithLogitsLoss()`             | Gabung `sigmoid + BCE` (lebih stabil)                               |\n",
    "| Cross Entropy           | `nn.CrossEntropyLoss()`              | Klasifikasi multi-kelas (output **tanpa softmax**, langsung logits) |\n",
    "| Negative Log Likelihood | `nn.NLLLoss()`                       | Digunakan bersama `log_softmax`                                     |\n",
    "| KL Divergence           | `nn.KLDivLoss()`                     | Distribusi probabilitas (output log dan target probabilitas)        |\n",
    "| Hinge Loss (manual)     | Custom `F.relu(1 - y_true * y_pred)` | SVM-style loss                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe9a61",
   "metadata": {},
   "source": [
    "| Nama Optimizer | Cara Memanggil                                         | Kegunaan                                                 |\n",
    "| -------------- | ------------------------------------------------------ | -------------------------------------------------------- |\n",
    "| Stochastic GD  | `optim.SGD(model.parameters(), lr=0.01)`               | Dasar, bisa dengan momentum                              |\n",
    "| SGD + Momentum | `optim.SGD(model.parameters(), lr=0.01, momentum=0.9)` | Lebih cepat konvergen                                    |\n",
    "| Adam           | `optim.Adam(model.parameters(), lr=0.001)`             | Umum & powerful, adaptif learning rate                   |\n",
    "| AdamW          | `optim.AdamW(model.parameters(), lr=0.001)`            | Adam dengan weight decay terpisah (untuk Transformer)    |\n",
    "| RMSprop        | `optim.RMSprop(model.parameters(), lr=0.001)`          | Baik untuk data sekuensial / non-stasioner               |\n",
    "| Adagrad        | `optim.Adagrad(model.parameters(), lr=0.01)`           | Learning rate per parameter                              |\n",
    "| Adadelta       | `optim.Adadelta(model.parameters())`                   | Tidak perlu learning rate awal                           |\n",
    "| LBFGS          | `optim.LBFGS(model.parameters())`                      | Untuk optimisasi convex (jarang digunakan di deep model) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000385e6",
   "metadata": {},
   "source": [
    "**Matrik Regresi**\n",
    "| Metrik                         | Kelas / Fungsi                                                   | Penjelasan                                             |\n",
    "| --------------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------ |\n",
    "| **MSE (Mean Squared Error)**      | `torchmetrics.regression.MeanSquaredError()`                     | Rata-rata kuadrat error antara prediksi dan target     |\n",
    "| **MAE (Mean Absolute Error)**     | `torchmetrics.regression.MeanAbsoluteError()`                    | Rata-rata nilai absolut selisih                        |\n",
    "| **MAPE**                          | `torchmetrics.regression.MeanAbsolutePercentageError()`          | Persentase kesalahan absolut rata-rata                 |\n",
    "| **SMAPE**                         | `torchmetrics.regression.SymmetricMeanAbsolutePercentageError()` | Versi simetris dari MAPE                               |\n",
    "| **MSLE (Mean Squared Log Error)** | `torchmetrics.regression.MeanSquaredLogError()`                  | Untuk data dengan skala log atau distribusi log-normal |\n",
    "| **Explained Variance**            | `torchmetrics.regression.ExplainedVariance()`                    | Variansi yang dijelaskan oleh model                    |\n",
    "| **R2 Score (R-squared)**          | `torchmetrics.regression.R2Score()`                              | Koefisien determinasi, 1 artinya model sempurna        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608167c2",
   "metadata": {},
   "source": [
    "**Metrik Klasifikasi Binnary**\n",
    "| Metrik            | Import                                             | Memanggil          | `y_pred` Format    |  Penjelasan                              |\n",
    "| -------------------- | ---------------------------------------------------------------- | -------------------------- | --------------------- | ------------------------------------------ |\n",
    "| Accuracy             | `from torchmetrics.classification import BinaryAccuracy`         | `BinaryAccuracy()`         | Probabilitas (0–1)    | Persentase prediksi benar                  |\n",
    "| Precision            | `from torchmetrics.classification import BinaryPrecision`        | `BinaryPrecision()`        | Probabilitas          | Ketepatan: TP / (TP + FP)                  |\n",
    "| Recall               | `from torchmetrics.classification import BinaryRecall`           | `BinaryRecall()`           | Probabilitas          | Sensitivitas: TP / (TP + FN)               |\n",
    "| F1 Score             | `from torchmetrics.classification import BinaryF1Score`          | `BinaryF1Score()`          | Probabilitas          | Harmonik dari precision dan recall         |\n",
    "| AUROC                | `from torchmetrics.classification import BinaryAUROC`            | `BinaryAUROC()`            | Probabilitas          | Area under ROC curve                       |\n",
    "| AUPRC (AvgPrecision) | `from torchmetrics.classification import BinaryAveragePrecision` | `BinaryAveragePrecision()` | Probabilitas          | Area under Precision-Recall Curve          |\n",
    "| Specificity          | `from torchmetrics.classification import BinarySpecificity`      | `BinarySpecificity()`      | Probabilitas          | True Negative Rate: TN / (TN + FP)         |\n",
    "| Matthews CorrCoef    | `from torchmetrics.classification import BinaryMatthewsCorrCoef` | `BinaryMatthewsCorrCoef()` | Probabilitas          | Metrik korelasi antara prediksi dan target |\n",
    "| Confusion Matrix     | `from torchmetrics.classification import BinaryConfusionMatrix`  | `BinaryConfusionMatrix()`  | Probabilitas / Logits | Matriks: TP, TN, FP, FN                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861443cb",
   "metadata": {},
   "source": [
    "**Metrik Klasifikasi Multiclass**\n",
    "| Metrik         | Import                                                 | Memanggil                           | `y_pred` Format     | Penjelasan                                           |\n",
    "| ----------------- | -------------------------------------------------------------------- | ------------------------------------------- | ---------------------- | ------------------------------------------------------- |\n",
    "| Accuracy          | `from torchmetrics.classification import MulticlassAccuracy`         | `MulticlassAccuracy(num_classes=N)`         | Logits (tanpa softmax) | Akurasi total: jumlah benar dibagi total                |\n",
    "| Precision         | `from torchmetrics.classification import MulticlassPrecision`        | `MulticlassPrecision(num_classes=N)`        | Logits / Probabilitas  | Ketepatan prediksi setiap kelas                         |\n",
    "| Recall            | `from torchmetrics.classification import MulticlassRecall`           | `MulticlassRecall(num_classes=N)`           | Logits / Probabilitas  | Sensitivitas: menangkap kelas benar                     |\n",
    "| F1 Score          | `from torchmetrics.classification import MulticlassF1Score`          | `MulticlassF1Score(num_classes=N)`          | Logits / Probabilitas  | Harmonik precision & recall; bisa `average='macro'` dll |\n",
    "| AUROC             | `from torchmetrics.classification import MulticlassAUROC`            | `MulticlassAUROC(num_classes=N)`            | Probabilitas (0–1)     | Area ROC untuk multi-kelas                              |\n",
    "| AUPRC             | `from torchmetrics.classification import MulticlassAveragePrecision` | `MulticlassAveragePrecision(num_classes=N)` | Probabilitas           | Area precision-recall per kelas                         |\n",
    "| Cohen’s Kappa     | `from torchmetrics.classification import MulticlassCohenKappa`       | `MulticlassCohenKappa(num_classes=N)`       | Logits / Probabilitas  | Evaluasi kesepakatan antar prediksi dan target          |\n",
    "| Matthews CorrCoef | `from torchmetrics.classification import MulticlassMatthewsCorrCoef` | `MulticlassMatthewsCorrCoef(num_classes=N)` | Logits / Probabilitas  | Metrik korelasi antara prediksi dan target kelas        |\n",
    "| Confusion Matrix  | `from torchmetrics.classification import MulticlassConfusionMatrix`  | `MulticlassConfusionMatrix(num_classes=N)`  | Logits / Probabilitas  | Matriks NxN: label vs prediksi                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0d7a2",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44722532",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825f5b7",
   "metadata": {},
   "source": [
    "| Fungsi                  | Keterangan                     |\n",
    "| ----------------------- | ------------------------------ |\n",
    "| `F.relu(x)`             | ReLU activation                |\n",
    "| `F.leaky_relu(x)`       | Leaky ReLU                     |\n",
    "| `F.sigmoid(x)`          | Sigmoid                        |\n",
    "| `F.tanh(x)`             | Tanh                           |\n",
    "| `F.elu(x)`              | Exponential Linear Unit        |\n",
    "| `F.gelu(x)`             | Gaussian Error Linear Unit     |\n",
    "| `F.selu(x)`             | Scaled Exponential Linear Unit |\n",
    "| `F.softmax(x, dim)`     | Softmax over dim               |\n",
    "| `F.log_softmax(x, dim)` | Log-Softmax                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3562b3",
   "metadata": {},
   "source": [
    "| Fungsi                             | Keterangan                                   |\n",
    "| ---------------------------------- | -------------------------------------------- |\n",
    "| `F.cross_entropy(output, target)`  | Cross-entropy loss (klasifikasi multi kelas) |\n",
    "| `F.binary_cross_entropy(out, tgt)` | Binary cross entropy loss                    |\n",
    "| `F.mse_loss(out, tgt)`             | Mean squared error loss                      |\n",
    "| `F.nll_loss(out, tgt)`             | Negative log-likelihood loss                 |\n",
    "| `F.l1_loss(out, tgt)`              | Mean absolute error                          |\n",
    "| `F.smooth_l1_loss(out, tgt)`       | Huber loss                                   |\n",
    "| `F.kl_div(out, tgt)`               | Kullback-Leibler divergence                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88ee0e",
   "metadata": {},
   "source": [
    "| Fungsi                         | Keterangan               |\n",
    "| ------------------------------ | ------------------------ |\n",
    "| `F.conv1d`, `conv2d`, `conv3d` | Operasi konvolusi manual |\n",
    "| `F.max_pool1d`, `max_pool2d`   | Max pooling              |\n",
    "| `F.avg_pool1d`, `avg_pool2d`   | Average pooling          |\n",
    "| `F.adaptive_avg_pool2d`        | Adaptive pooling         |\n",
    "| `F.interpolate(x)`             | Upsampling/downsampling  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2a7c8",
   "metadata": {},
   "source": [
    "| Fungsi                  | Keterangan                   |\n",
    "| ----------------------- | ---------------------------- |\n",
    "| `F.batch_norm`          | Batch normalization          |\n",
    "| `F.layer_norm`          | Layer normalization          |\n",
    "| `F.instance_norm`       | Instance normalization       |\n",
    "| `F.group_norm`          | Group normalization          |\n",
    "| `F.local_response_norm` | Local response normalization |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdae82f",
   "metadata": {},
   "source": [
    "| Fungsi            | Keterangan                     |\n",
    "| ----------------- | ------------------------------ |\n",
    "| `F.dropout`       | Dropout biasa                  |\n",
    "| `F.dropout2d`     | Dropout untuk input 4D (CNN)   |\n",
    "| `F.dropout3d`     | Dropout untuk input 5D         |\n",
    "| `F.alpha_dropout` | Dropout untuk SELU aktivasinya |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bde80",
   "metadata": {},
   "source": [
    "| Fungsi                      | Keterangan             |\n",
    "| --------------------------- | ---------------------- |\n",
    "| `F.linear(x, w, b)`         | Operasi linear         |\n",
    "| `F.embedding(x, weight)`    | Lookup tabel embedding |\n",
    "| `F.one_hot(x, num_classes)` | Konversi ke one-hot    |\n",
    "| `F.normalize(x)`            | Normalisasi vektor     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea224ea",
   "metadata": {},
   "source": [
    "| Fungsi                        | Keterangan        |\n",
    "| ----------------------------- | ----------------- |\n",
    "| `F.pairwise_distance(x1, x2)` | Jarak Euclidean   |\n",
    "| `F.cosine_similarity(x1, x2)` | Kemiripan kosinus |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34bb10e",
   "metadata": {},
   "source": [
    "| Fungsi                                               | Keterangan                       |\n",
    "| ---------------------------------------------------- | -------------------------------- |\n",
    "| `F.pad(input, pad)`                                  | Padding manual                   |\n",
    "| `F.unfold(input, kernel_size)`                       | Ekstrak patch                    |\n",
    "| `F.fold(input, output_size)`                         | Balik dari unfold                |\n",
    "| `F.grid_sample(input, grid)`                         | Sampling berbasis koordinat grid |\n",
    "| `F.affine_grid(theta, size)`                         | Grid transformasi afine          |\n",
    "| `F.binary_cross_entropy_with_logits(logits, target)` | Binary cross entropy dari logits |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3c368",
   "metadata": {},
   "source": [
    "# **Struktur Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eee2b3",
   "metadata": {},
   "source": [
    "`nn.<Layer>(Fitur, Neuron)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c244a29",
   "metadata": {},
   "source": [
    "- **Fitur di pytorch harus definisikan berulang-ulang berbeda dengan tensorflow yg didefinisikan diawal**\n",
    "- **Konsepnay seperti silang, fitur yg kita isi dilayer selanjutnya ada jumlah neurn pada layer sebelumnya**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e88f3",
   "metadata": {},
   "source": [
    "**Untuk menerima Fitur = (A, B) di pytorch berbeda dengan tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5b504",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hitung input size setelah flatten\n",
    "        self.input_dim = input_shape[0] * input_shape[1]\n",
    "\n",
    "        # Layer-layer\n",
    "        self.fc1 = nn.Linear(self.input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)        # Flatten secara manual\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = CustomModel(input_shape=(100, 40), output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d82cf",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer flatten otomatis\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Hitung input size setelah flatten\n",
    "        self.input_dim = input_shape[0] * input_shape[1]\n",
    "\n",
    "        # Layer-layer lainnya\n",
    "        self.fc1 = nn.Linear(self.input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)         # Gunakan nn.Flatten()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = CustomModel(input_shape=(100, 40), output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bf28f",
   "metadata": {},
   "source": [
    "1. **Sequential**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ea735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 64),   # Layer 1\n",
    "    nn.ReLU(),            # Aktivasi 1\n",
    "\n",
    "    nn.Linear(64, 32),    # Layer 2\n",
    "    nn.ReLU(),            # Aktivasi 2\n",
    "\n",
    "    nn.Linear(32, 10),    # Output Layer\n",
    "    nn.Softmax(dim=1)     # Aktivasi output (klasifikasi multi-kelas)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ff023",
   "metadata": {},
   "source": [
    "2. **Object-Oriented Programming (OOP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ee468",
   "metadata": {},
   "source": [
    "**Murni**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d771df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ini tempat mendefinisikan\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # ini tempat eksekui, lihat bahwa konsepnya mirip API fungsional tensorflow -> x = ___(x)\n",
    "        x = self.conv1(x)  \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86ec85",
   "metadata": {},
   "source": [
    "**Versi dengan F** : Fungsi aktivasi didefinisikan dengan F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700853a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Tempat mendefinisikan\n",
    "        self.fc1 = nn.Linear(100, 64)  # Layer pertama\n",
    "        self.fc2 = nn.Linear(64, 32)   # Layer kedua\n",
    "        self.fc3 = nn.Linear(32, 10)   # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tempat eksekusi\n",
    "        # dengan F, kita tidak perlu mendefinisikan fungsi aktivasi diatas, bisa dibuat dengan F dibawah\n",
    "        x = self.fc1(x)        # Layer 1\n",
    "        x = F.relu(x)          # Aktivasi 1\n",
    "        x = self.fc2(x)        # Layer 2\n",
    "        x = F.relu(x)          # Aktivasi 2\n",
    "        x = self.fc3(x)        # Layer 3\n",
    "        x = F.softmax(x, dim=1)  # Aktivasi Output (klasifikasi multi-kelas)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b7bc0",
   "metadata": {},
   "source": [
    "- **Two-Branch Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f74cdf",
   "metadata": {},
   "source": [
    "`torch.cat` : Menggabungkan dua tensor dari dua \"cabang\" berbeda menjadi satu tensor sepanjang dimensi tertentu (biasanya dim=1 / axis=1) untuk bisa diproses bersama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3d675",
   "metadata": {},
   "source": [
    "**Versi gabungan `Squential`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input 1 \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        # input 2\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Gabungan kedua branch\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1_out = self.branch1(x1)\n",
    "        x2 = x2.unsqueeze(1)  # reshape (batch, 1, features) for Conv1d\n",
    "        x2_out = self.branch2(x2)\n",
    "        merged = torch.cat((x1_out, x2_out), dim=1)\n",
    "        return self.fc(merged)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f8812",
   "metadata": {},
   "source": [
    "**Murni OOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranchNetOOP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Branch 1: Untuk input fitur numerik\n",
    "        self.linear1 = nn.Linear(10, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Branch 2: Untuk input sensor/gambar 1D\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully Connected setelah penggabungan dua branch\n",
    "        self.fc1 = nn.Linear(64 + 32, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Branch 1\n",
    "        x1 = self.linear1(x1)\n",
    "        x1 = self.relu1(x1)\n",
    "        x1 = self.bn1(x1)\n",
    "\n",
    "        # Branch 2\n",
    "        x2 = x2.unsqueeze(1)  # (batch, 1, features)\n",
    "        x2 = self.conv1(x2)\n",
    "        x2 = self.relu2(x2)\n",
    "        x2 = self.pool(x2)\n",
    "        x2 = self.flatten(x2)\n",
    "\n",
    "        # Concatenate kedua branch\n",
    "        merged = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Fully Connected akhir\n",
    "        out = self.fc1(merged)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793fc9e",
   "metadata": {},
   "source": [
    "- **Residual Conection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe51fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResidualModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc_out = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = F.relu(self.fc1(x))      # simpan residual setelah aktivasi\n",
    "        out = F.relu(self.fc2(residual))\n",
    "        out = out + residual                # residual connection\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7c33b",
   "metadata": {},
   "source": [
    "- **Multi-Head Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ed8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.regression_head = nn.Linear(64, 1)     # Output regresi\n",
    "        self.classification_head = nn.Linear(64, 3) # Output klasifikasi (misal 3 kelas)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        out_regression = self.regression_head(shared_out)\n",
    "        out_classification = self.classification_head(shared_out)\n",
    "        return out_regression, out_classification   # Terlihat bahwa ada 2 return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8ca52",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce78f1b",
   "metadata": {},
   "source": [
    "**Style Menulis Code di Forward :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b0e8a",
   "metadata": {},
   "source": [
    "- Langsung digabung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.fc1 = nn.Linear(20, 128)\n",
    "    self.bn1 = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.drop = nn.Dropout(0.3)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = self.drop(self.relu(self.bn1(self.fc1(x)))) # yg paling dalam duluan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6375bc1",
   "metadata": {},
   "source": [
    "- Didefinisikan dibawahnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9822513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTerbuka(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)         # Step 1: Linear transform\n",
    "        x = self.bn1(x)         # Step 2: Batch normalization\n",
    "        x = self.relu(x)        # Step 3: Activation\n",
    "        x = self.drop(x)        # Step 4: Dropout\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f2689",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704cdc6",
   "metadata": {},
   "source": [
    "**Style Arsitektur Model pada Input, Output, Hidden:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9afd21",
   "metadata": {},
   "source": [
    "- Didefinisikan Langsung pada Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86979cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelFixed5_OOP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hardcoded dimensions\n",
    "        # Layer 1: 20 -> 128\n",
    "        self.fc1 = nn.Linear(20, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        # Layer 2: 128 -> 64\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "\n",
    "        # Layer 3: 64 -> 32\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "\n",
    "        # Layer 4: 32 -> 16\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        self.drop4 = nn.Dropout(0.3)\n",
    "\n",
    "        # Output layer: 16 -> 1\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = DeepModelFixed5_OOP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69d110",
   "metadata": {},
   "source": [
    "- Dibuat Menjadi Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02da7ee",
   "metadata": {},
   "source": [
    "Tanpa sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelParam5_OOP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # 5 Layer : 4 Hidden Layer + 1 Output\n",
    "        assert len(hidden_dims) == 4 , \"Model harus punya 4 hidden layer (total 5 layer termasuk output)\"\n",
    "\n",
    "        # Layer 1\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 2\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 3\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n",
    "        self.drop3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 4\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dims[3])\n",
    "        self.drop4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Output layer (layer ke-5)\n",
    "        self.fc5 = nn.Linear(hidden_dims[3], output_dim)\n",
    "\n",
    "        # Activation (satu objek, dipakai berulang)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        # Layer 4\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        # Output layer \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = DeepModelParam5_OOP(\n",
    "    input_dim=20,\n",
    "    hidden_dims=[128, 64, 32, 16],\n",
    "    output_dim=1,\n",
    "    dropout_rate=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4adf2b",
   "metadata": {},
   "source": [
    "Dengan sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c22d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelParam5(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # 5 Layer : 4 Hidden Layer + 1 Output\n",
    "        assert len(hidden_dims) == 4 , \"Model harus punya 4 hidden layer (total 5 layer termasuk output)\"\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = DeepModelParam5_OOP(\n",
    "    input_dim=20,\n",
    "    hidden_dims=[128, 64, 32, 16],\n",
    "    output_dim=1,\n",
    "    dropout_rate=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bad7d",
   "metadata": {},
   "source": [
    "# **Tingkatan**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304a7da",
   "metadata": {},
   "source": [
    "**Sederhana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bdb3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 16.5367\n",
      "Epoch 10, Loss: 0.4294\n",
      "Epoch 20, Loss: 0.0126\n",
      "Epoch 30, Loss: 0.0017\n",
      "Epoch 40, Loss: 0.0014\n",
      "Epoch 50, Loss: 0.0013\n",
      "Epoch 60, Loss: 0.0012\n",
      "Epoch 70, Loss: 0.0011\n",
      "Epoch 80, Loss: 0.0011\n",
      "Epoch 90, Loss: 0.0010\n",
      "tensor([[9.9473]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset sederhana\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Model Linear\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Loss dan Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Prediksi\n",
    "print(model(torch.tensor([[5.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f21bd",
   "metadata": {},
   "source": [
    "**Menegah**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8984592",
   "metadata": {},
   "source": [
    "**Advanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.rand(5000, embed_dim))\n",
    "        encoder_layers = TransformerEncoderLayer(embed_dim, nhead, hidden_dim)\n",
    "        self.transformer = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoder[:x.size(1)]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # global average pooling\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd095858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Branch 1\n",
    "        self.dense1_branch1 = nn.Linear(3, 64)\n",
    "        self.bn1_branch1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1_branch1 = nn.Dropout(0.3)\n",
    "        self.dense_residual_branch1 = nn.Linear(64, 64)\n",
    "        self.dense2_branch1 = nn.Linear(64, 32)\n",
    "\n",
    "        ## Branch 2 - Conv1D\n",
    "        self.conv1_branch2 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=2, padding=1)\n",
    "        self.bn1_branch2 = nn.BatchNorm1d(32)\n",
    "        self.maxpool_branch2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dense1_branch2 = nn.Linear(32 * 2, 64)  # Output flatten setelah pooling\n",
    "        self.dropout1_branch2 = nn.Dropout(0.3)\n",
    "\n",
    "        ## Branch 2 - Dense Parallel\n",
    "        self.dense_parallel_branch2 = nn.Linear(4, 48)\n",
    "        self.bn_parallel_branch2 = nn.BatchNorm1d(48)\n",
    "\n",
    "        ## Attention\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=32 + 64 + 48, num_heads=4)\n",
    "\n",
    "        ## After Attention\n",
    "        self.dense_post_attention = nn.Linear(32 + 64 + 48, 128)\n",
    "        self.bn_post_attention = nn.BatchNorm1d(128)\n",
    "        self.dropout_post_attention = nn.Dropout(0.4)\n",
    "        self.dense_residual_post = nn.Linear(128, 128)\n",
    "\n",
    "        ## Output\n",
    "        self.output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        ### Branch 1\n",
    "        x1 = F.relu(self.dense1_branch1(input1))\n",
    "        x1 = self.bn1_branch1(x1)\n",
    "        x1 = self.dropout1_branch1(x1)\n",
    "        x1_res = F.relu(self.dense_residual_branch1(x1))\n",
    "        x1 = x1 + x1_res  # Residual connection\n",
    "        x1 = F.relu(self.dense2_branch1(x1))\n",
    "\n",
    "        ### Branch 2 - Conv1D\n",
    "        x2 = input2.unsqueeze(1)  # shape: (batch, 1, 4)\n",
    "        x2 = F.relu(self.conv1_branch2(x2))  # (batch, 32, 4)\n",
    "        x2 = self.bn1_branch2(x2)\n",
    "        x2 = self.maxpool_branch2(x2)  # (batch, 32, 2)\n",
    "        x2 = x2.view(x2.size(0), -1)  # flatten\n",
    "        x2 = F.relu(self.dense1_branch2(x2))\n",
    "        x2 = self.dropout1_branch2(x2)\n",
    "\n",
    "        ### Branch 2 - Parallel Dense\n",
    "        x2_parallel = F.relu(self.dense_parallel_branch2(input2))\n",
    "        x2_parallel = self.bn_parallel_branch2(x2_parallel)\n",
    "\n",
    "        ### Gabung branch2\n",
    "        x2_merged = torch.cat([x2, x2_parallel], dim=1)  # (batch, 64+48)\n",
    "\n",
    "        ### Gabung seluruh branch\n",
    "        merged = torch.cat([x1, x2_merged], dim=1)  # (batch, 32+64+48)\n",
    "\n",
    "        ### Attention\n",
    "        merged_attn_input = merged.unsqueeze(1)  # (batch, 1, features)\n",
    "        attn_output = self.attention(merged_attn_input).squeeze(1)  # (batch, features)\n",
    "\n",
    "        ### Post-attention processing\n",
    "        x = F.relu(self.dense_post_attention(attn_output))\n",
    "        x = self.bn_post_attention(x)\n",
    "        x = self.dropout_post_attention(x)\n",
    "        x_res = F.relu(self.dense_residual_post(x))\n",
    "        x = x + x_res  # residual connection\n",
    "\n",
    "        ### Output\n",
    "        output = torch.sigmoid(self.output(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "# Dummy dataset\n",
    "input1 = torch.randn(1000, 3)\n",
    "input2 = torch.randn(1000, 4)\n",
    "labels = torch.randint(0, 2, (1000,)).float()\n",
    "\n",
    "# Split train & val\n",
    "train_idx, val_idx = train_test_split(range(1000), test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(input1[train_idx], input2[train_idx], labels[train_idx])\n",
    "val_dataset = TensorDataset(input1[val_idx], input2[val_idx], labels[val_idx])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Inisialisasi\n",
    "model = ComplexModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input1_batch, input2_batch, y_batch in train_loader:\n",
    "        input1_batch, input2_batch, y_batch = input1_batch.to(device), input2_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(input1_batch, input2_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input1_batch, input2_batch, y_batch in val_loader:\n",
    "            input1_batch, input2_batch, y_batch = input1_batch.to(device), input2_batch.to(device), y_batch.to(device)\n",
    "            preds = model(input1_batch, input2_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            preds_binary = (preds > 0.5).float()\n",
    "            correct += (preds_binary == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    acc = correct / total\n",
    "    print(f\"          Val Loss: {val_loss / len(val_loader):.4f}, Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff31cf",
   "metadata": {},
   "source": [
    "# **PyTorch Lightning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1570455",
   "metadata": {},
   "source": [
    "high-level wrapper untuk PyTorch yang membantu kamu membuat model deep learning lebih bersih, terstruktur, dan scalable, tanpa mengubah core PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f33b7",
   "metadata": {},
   "source": [
    "| Fitur                          | PyTorch Biasa                              | PyTorch Lightning           |\n",
    "| ------------------------------ | ------------------------------------------ | --------------------------- |\n",
    "| Looping Training               | Ditulis manual (`for epoch in range(...)`) | Otomatis oleh `Trainer`     |\n",
    "| Kode Training dan Model Campur | Ya                                         | Tidak, dipisah dengan jelas |\n",
    "| Logging Manual                 | Ya                                         | Otomatis via `self.log()`   |\n",
    "| Multi-GPU, TPU                 | Manual dan kompleks                        | Sangat mudah dengan 1 baris |\n",
    "| Checkpointing                  | Manual                                     | Otomatis                    |\n",
    "| Monitoring Loss, Metric        | Harus ditulis sendiri                      | Otomatis                    |\n",
    "| Readability                    | Bisa berantakan                            | Bersih dan modular          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd594a45",
   "metadata": {},
   "source": [
    "- Versi lama : `import pytorch_lightning as pl`\n",
    "- Versi baru : `import lightning as l`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef73dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Biasa\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "### Pytorch Lightning\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(10, 1)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "model = LitModel()\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a5304",
   "metadata": {},
   "source": [
    "**style penggunaan lightning bisa digabung menjadi 1 class bersamaan dengan model deep learning atau dipisah**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a50c2",
   "metadata": {},
   "source": [
    "> **Versi Gabung**\n",
    "```python \n",
    "class LitModelOOP(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "model = LitModelOOP(input_dim=20, hidden_dim=64, output_dim=1, lr=1e-3)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.validate(model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a19f5",
   "metadata": {},
   "source": [
    "> **Versi Pisah**\n",
    "```python\n",
    "class MyOOPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LitModelSeparate(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "model = MyOOPModel(input_dim=20, hidden_dim=64, output_dim=1)\n",
    "lit_model = LitModelSeparate(model=model, lr=1e-3)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.validate(model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a66b9c",
   "metadata": {},
   "source": [
    "# **Arsitektur Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa4baa",
   "metadata": {},
   "source": [
    "**Model**\n",
    "| No | Model                           | Modul yang Diimpor                         | Cara Mengimpor Modelnya Lengkap                                                      |\n",
    "| -- | ------------------------------- | ------------------------------------------ | ------------------------------------------------------------------------------------ |\n",
    "| 1  | Multilayer Perceptron (MLP)     | `import torch.nn as nn`                    | **Custome**          |\n",
    "| 2  | Convolutional Neural Net (CNN)  | `import torch.nn as nn`                    | **Custome** |\n",
    "| 3  | Recurrent Neural Net (RNN)      | `from torch.nn import RNN`                 | `rnn = RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)`           |\n",
    "| 4  | Long Short-Term Memory (LSTM)   | `from torch.nn import LSTM`                | `lstm = LSTM(input_size=10, hidden_size=50, num_layers=2, batch_first=True)`         |\n",
    "| 5  | Gated Recurrent Unit (GRU)      | `from torch.nn import GRU`                 | `gru = GRU(input_size=10, hidden_size=50, num_layers=2, batch_first=True)`           |\n",
    "| 6  | Transformer                     | `from torch.nn import Transformer`         | `transformer = Transformer(d_model=512, nhead=8, num_encoder_layers=6)`              |\n",
    "| 7 | ResNet-18 (Pretrained)          | `from torchvision import models`           | `resnet = models.resnet18(pretrained=True)`                                          |\n",
    "| 8 | VGG-16 (Pretrained)             | `from torchvision import models`           | `vgg = models.vgg16(pretrained=True)`                                                |\n",
    "| 9 | MobileNet (Pretrained)          | `from torchvision import models`           | `mobilenet = models.mobilenet_v2(pretrained=True)`                                   |\n",
    "| 10 | EfficientNet (Pretrained)       | `from torchvision import models`           | `efficientnet = models.efficientnet_b0(pretrained=True)`                             |\n",
    "| 11 | DenseNet (Pretrained)           | `from torchvision import models`           | `densenet = models.densenet121(pretrained=True)`                                     |\n",
    "| 12 | Vision Transformer (ViT)        | `from torchvision.models import vit_b_16`  | `model = vit_b_16(pretrained=True)`                                                  |\n",
    "| 13 | Swin Transformer (ViT)          | `from torchvision.models import swin_v2_b` | `model = swin_v2_b(pretrained=True)`                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf674b",
   "metadata": {},
   "source": [
    "| No | Model                           | Modul yang Diimpor                         | Cara Mengimpor Modelnya Lengkap                                                      |\n",
    "| -- | ------------------------------- | ------------------------------------------ | ------------------------------------------------------------------------------------ |\n",
    "| 1 | BERT (NLP)                      | `from transformers import BertModel`       | `model = BertModel.from_pretrained('bert-base-uncased')`                             |\n",
    "| 2 | GPT-2 (NLP)                     | `from transformers import GPT2Model`       | `model = GPT2Model.from_pretrained('gpt2')`                                          |\n",
    "| 3 | T5 (NLP)                        | `from transformers import T5Model`         | `model = T5Model.from_pretrained('t5-small')`                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c522354",
   "metadata": {},
   "source": [
    "**MLP (Multi-Layer Perceptron)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6ede5",
   "metadata": {},
   "source": [
    "**CNN (Convolutional Neural Network)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # input: (3,32,32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)                           # output: (32,16,16)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # (64,16,16)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (32,16,16)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (64,8,8)\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd544a",
   "metadata": {},
   "source": [
    "**AutoEncoder**\n",
    "\n",
    "Fungsi : Mengkompres data dan rekonstruksi ulang. Berguna untuk noise removal, dimensionality reduction, anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_fc1 = nn.Linear(28*28, 256)\n",
    "        self.encoder_fc2 = nn.Linear(256, 64)\n",
    "\n",
    "        self.decoder_fc1 = nn.Linear(64, 256)\n",
    "        self.decoder_fc2 = nn.Linear(256, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = F.relu(self.encoder_fc1(x))\n",
    "        encoded = F.relu(self.encoder_fc2(encoded))\n",
    "\n",
    "        decoded = F.relu(self.decoder_fc1(encoded))\n",
    "        decoded = torch.sigmoid(self.decoder_fc2(decoded))\n",
    "        return decoded.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = AutoEncoder()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x_hat = self(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x_hat = self(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder():\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = ___()\n",
    "    train_set, val_set = random_split(dataset, [55000, 5000])\n",
    "    train_loader = DataLoader(train_set, batch_size=64)\n",
    "    val_loader = DataLoader(val_set, batch_size=64)\n",
    "\n",
    "    model = LitAutoEncoder()\n",
    "    trainer = L.Trainer(max_epochs=5, accelerator=\"cpu\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345048",
   "metadata": {},
   "source": [
    "**U-Net (Semantic Segmentation)**\n",
    "\n",
    "Fungsi : Digunakan untuk segmentasi gambar pixel-wise \n",
    "\n",
    "Arsitektur :\n",
    "- Encoder (CNN biasa, downsampling)\n",
    "- Bottleneck\n",
    "- Decoder (upsampling + skip connection dari encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class UNetOOP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = UNetBlock(1, 64)    # Model diatas\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = UNetBlock(64, 128)  # Model diatas\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bottleneck = UNetBlock(128, 256)   # Model diatas\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = UNetBlock(256, 128) # Model diatas\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = UNetBlock(128, 64)  # Model diatas\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.dec2(torch.cat([self.up2(b), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return torch.sigmoid(self.final(d1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUNet(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = UNetOOP()  # Model Utama\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8be7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet():\n",
    "    dataset = ___\n",
    "    train_set, val_set = random_split(dataset, [160, 40])\n",
    "    train_loader = DataLoader(train_set, batch_size=8)\n",
    "    val_loader = DataLoader(val_set, batch_size=8)\n",
    "\n",
    "    model = LitUNet()\n",
    "    trainer = L.Trainer(max_epochs=5, accelerator=\"cpu\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e5e2a",
   "metadata": {},
   "source": [
    "**Siamese Network (Similarity Learning)**\n",
    "\n",
    "Fungsi : Digunakan untuk membandingkan dua input\n",
    "\n",
    "Arsitektur :\n",
    "- Dua cabang dengan parameter sharing\n",
    "- Loss: contrastive loss atau triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(16*13*13, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = SiameseBackbone()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.backbone(x1)\n",
    "        out2 = self.backbone(x2)\n",
    "        return out1, out2\n",
    "\n",
    "def contrastive_loss(out1, out2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(out1, out2)\n",
    "    loss = label * distance**2 + (1 - label) * F.relu(margin - distance)**2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSiamese(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = SiameseNet()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2, label = batch\n",
    "        out1, out2 = self(x1, x2)\n",
    "        loss = contrastive_loss(out1, out2, label)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese():\n",
    "    dataset = DummySiameseDataset()\n",
    "    train_loader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    model = LitSiamese()\n",
    "    trainer = L.Trainer(max_epochs=5, accelerator=\"cpu\")\n",
    "    trainer.fit(model, train_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_unet()\n",
    "    train_siamese()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5776a",
   "metadata": {},
   "source": [
    "**GAN (Generative Adversarial Network)**\n",
    "\n",
    "Fungsi : Model generatif yang mampu membuat gambar, audio, atau data sintetik lainnya.\n",
    "\n",
    "Arsitektur :\n",
    "- Generator: membuat gambar palsu\n",
    "- Discriminator: membedakan gambar asli dan palsu\n",
    "- Training: Adversarial (minimax game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(noise_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 28 * 28)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return x.view(-1, 1, 28, 28)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        return torch.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitGAN(L.LightningModule):\n",
    "    def __init__(self, noise_dim=100, lr=2e-4):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(noise_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_imgs, _ = batch\n",
    "        batch_size = real_imgs.size(0)\n",
    "        valid = torch.ones(batch_size, 1, device=self.device)\n",
    "        fake = torch.zeros(batch_size, 1, device=self.device)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # Train generator\n",
    "            z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "            gen_imgs = self(z)\n",
    "            pred_fake = self.discriminator(gen_imgs)\n",
    "            g_loss = self.adversarial_loss(pred_fake, valid)\n",
    "            self.log(\"g_loss\", g_loss)\n",
    "            return g_loss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # Train discriminator\n",
    "            pred_real = self.discriminator(real_imgs)\n",
    "            real_loss = self.adversarial_loss(pred_real, valid)\n",
    "\n",
    "            z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "            gen_imgs = self(z).detach()\n",
    "            pred_fake = self.discriminator(gen_imgs)\n",
    "            fake_loss = self.adversarial_loss(pred_fake, fake)\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            self.log(\"d_loss\", d_loss)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(0.5, 0.999))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(0.5, 0.999))\n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = LitGAN()\n",
    "    trainer = L.Trainer(max_epochs=10, accelerator=\"cpu\")\n",
    "    trainer.fit(model, train_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_gan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad36da",
   "metadata": {},
   "source": [
    "**N-BEATS**\n",
    "\n",
    "Fungsi : membangun model yang mampu melakukan forecasting deret waktu secara akurasi tinggi dan interpretable, tanpa perlu informasi domain khusus seperti tanggal, musim, dsb (purely data-driven)\n",
    "\n",
    "Mekanisme :\n",
    "1. Stacked Residual Blocks : Beberapa blok (residual blocks) yang disusun secara berurutan, Setiap blok menerima input x lalu menghasilkan \n",
    "- Backcast → perkiraan bagian dari input yang bisa dijelaskan (sisa input setelah dipelajari)\n",
    "- Forecast → prediksi untuk masa depan \n",
    "\n",
    "Forecast dari semua blok dijumlahkan → total prediksi akhir.\n",
    "\n",
    "2. Decomposisi Backcast-Forecast : Setiap blok mempelajari dua hal yakni \n",
    "- Backcast → bagian input yang bisa dijelaskan (mirip autoencoder)\n",
    "- Forecast → bagian prediksi masa depan (apa yang kita inginkan)\n",
    "\n",
    "Model mencoba mempelajari residu (sisa input) di setiap blok, seperti mekanisme residual learning\n",
    "\n",
    "3. Basis Expansion : N-BEATS menggunakan ide basis functions (seperti polinomial atau Fourier)\n",
    "- Pada tiap blok, hasil theta akan digunakan untuk merekonstruksi forecast menggunakan basi\n",
    "- Dua jenis basis bawaan → Trend basis (polinomial: 1, t, t², …) → cocok untuk tren naik/turun & Seasonal basis (Fourier/sinus) → cocok untuk pola berulang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4adc35",
   "metadata": {},
   "source": [
    "```\n",
    "Input Series:\n",
    "     │\n",
    "     │\n",
    "     ├── Backcast Block1: ──[FC]──┐\n",
    "     │                            ▼\n",
    "     │                     Residual: x - backcast\n",
    "     │                           │\n",
    "     ├── Backcast Block2: ──[FC]─┐\n",
    "     │                           ...\n",
    "Forecast: Sum dari semua blok\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block dasar N-BEATS\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, theta_size, hidden_size=128, n_hidden=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.ModuleList([nn.Linear(input_size if i == 0 else hidden_size, hidden_size) for i in range(n_hidden)])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.theta = nn.Linear(hidden_size, theta_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = self.relu(layer(x))\n",
    "        theta = self.theta(x)\n",
    "        return theta\n",
    "\n",
    "# Block trend basis\n",
    "class TrendBlock(NBeatsBlock):\n",
    "    def __init__(self, input_size, forecast_size, hidden_size=128, n_hidden=4):\n",
    "        super().__init__(input_size, theta_size=forecast_size, hidden_size=hidden_size, n_hidden=n_hidden)\n",
    "        self.backcast_size = input_size\n",
    "        self.forecast_size = forecast_size\n",
    "\n",
    "        self.register_buffer(\"t\", torch.linspace(-1, 1, steps=forecast_size).unsqueeze(0))\n",
    "        self.basis = torch.stack([self.t ** i for i in range(forecast_size)], dim=-1)  # (1, forecast_size, forecast_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = super().forward(x)\n",
    "        forecast = torch.matmul(self.basis, theta.unsqueeze(-1)).squeeze(-1)  # shape (B, forecast_size)\n",
    "        backcast = x - forecast  # residual learning\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBeatsModel(L.LightningModule):\n",
    "    def __init__(self, input_size, forecast_size, hidden_size=128, n_blocks=3, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TrendBlock(input_size, forecast_size, hidden_size=hidden_size)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        forecast = 0\n",
    "        for block in self.blocks:\n",
    "            backcast, block_forecast = block(x)\n",
    "            x = backcast\n",
    "            forecast += block_forecast\n",
    "        return forecast\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76aa750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NBeatsModel(input_size=30, forecast_size=10, hidden_size=128, n_blocks=4, lr=1e-3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "    accelerator='auto',\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
